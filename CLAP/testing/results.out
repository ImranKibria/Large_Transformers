2024-06-20 10:54:05.160941: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2024-06-20 10:54:05.161312: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2024-06-20 10:54:05.568820: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-06-20 10:54:06.292624: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-06-20 10:54:08.524483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
/users/PAS2301/kibria5/.conda/envs/vit_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
Some weights of the model checkpoint at laion/clap-htsat-unfused were not used when initializing ClapAudioModelWithProjection: ['text_model.encoder.layer.8.output.LayerNorm.weight', 'text_model.encoder.layer.7.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.output.dense.bias', 'text_model.encoder.layer.11.intermediate.dense.bias', 'text_model.embeddings.LayerNorm.weight', 'text_model.encoder.layer.3.intermediate.dense.bias', 'text_model.encoder.layer.8.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.dense.bias', 'text_model.encoder.layer.4.attention.self.query.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.weight', 'text_model.encoder.layer.7.intermediate.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.weight', 'text_model.encoder.layer.9.output.dense.bias', 'text_model.encoder.layer.5.attention.output.LayerNorm.weight', 'text_projection.linear2.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.self.query.bias', 'text_model.encoder.layer.3.attention.self.value.weight', 'text_model.encoder.layer.8.attention.output.dense.bias', 'text_model.encoder.layer.10.attention.self.value.weight', 'text_model.encoder.layer.9.output.LayerNorm.weight', 'text_model.encoder.layer.3.output.LayerNorm.weight', 'text_model.pooler.dense.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.LayerNorm.bias', 'text_model.encoder.layer.11.attention.self.query.bias', 'text_model.encoder.layer.4.attention.output.dense.weight', 'text_model.encoder.layer.4.intermediate.dense.weight', 'text_model.encoder.layer.4.attention.self.query.weight', 'text_model.encoder.layer.4.intermediate.dense.bias', 'text_model.encoder.layer.11.attention.self.value.bias', 'text_model.encoder.layer.10.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.value.bias', 'text_model.encoder.layer.0.attention.self.query.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_model.encoder.layer.11.output.LayerNorm.bias', 'text_model.encoder.layer.2.output.dense.weight', 'text_model.encoder.layer.7.attention.self.key.weight', 'text_model.encoder.layer.9.attention.output.LayerNorm.bias', 'text_model.encoder.layer.0.attention.self.value.bias', 'text_model.encoder.layer.2.attention.output.dense.weight', 'text_model.encoder.layer.5.output.LayerNorm.bias', 'text_model.encoder.layer.3.attention.output.LayerNorm.bias', 'text_model.encoder.layer.8.output.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.weight', 'text_model.embeddings.word_embeddings.weight', 'text_model.encoder.layer.1.attention.output.dense.weight', 'text_model.encoder.layer.9.attention.output.dense.bias', 'text_model.encoder.layer.5.attention.self.value.weight', 'text_model.encoder.layer.1.intermediate.dense.bias', 'text_model.encoder.layer.6.attention.self.value.bias', 'text_model.encoder.layer.3.intermediate.dense.weight', 'text_model.encoder.layer.4.output.dense.bias', 'text_model.embeddings.position_embeddings.weight', 'text_model.encoder.layer.3.attention.self.key.weight', 'text_model.encoder.layer.4.attention.self.value.bias', 'text_model.embeddings.token_type_embeddings.weight', 'text_model.encoder.layer.5.output.dense.bias', 'text_model.encoder.layer.5.attention.output.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.weight', 'text_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.self.value.bias', 'text_model.encoder.layer.11.attention.self.key.bias', 'text_model.encoder.layer.5.attention.self.value.bias', 'text_model.encoder.layer.4.attention.self.value.weight', 'text_model.encoder.layer.2.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.self.key.bias', 'text_model.encoder.layer.11.attention.self.key.weight', 'text_model.encoder.layer.9.attention.self.query.bias', 'text_model.pooler.dense.weight', 'text_model.encoder.layer.8.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.value.weight', 'text_model.encoder.layer.8.attention.self.query.bias', 'text_model.encoder.layer.8.attention.self.query.weight', 'text_model.encoder.layer.7.attention.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.weight', 'text_model.encoder.layer.4.output.LayerNorm.weight', 'text_model.encoder.layer.5.intermediate.dense.bias', 'text_model.encoder.layer.3.attention.self.value.bias', 'logit_scale_a', 'text_model.encoder.layer.0.intermediate.dense.bias', 'text_model.encoder.layer.9.attention.self.value.bias', 'text_model.encoder.layer.7.attention.self.query.bias', 'text_model.encoder.layer.11.output.dense.bias', 'text_model.encoder.layer.6.attention.output.dense.weight', 'text_model.encoder.layer.10.output.dense.bias', 'text_model.encoder.layer.8.output.dense.bias', 'text_model.encoder.layer.7.attention.self.value.bias', 'text_model.encoder.layer.5.intermediate.dense.weight', 'text_model.encoder.layer.1.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.output.dense.bias', 'text_projection.linear1.bias', 'text_model.encoder.layer.4.attention.self.key.weight', 'text_model.encoder.layer.0.attention.self.value.weight', 'text_model.encoder.layer.6.attention.self.value.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.intermediate.dense.weight', 'text_model.encoder.layer.9.attention.self.query.weight', 'text_model.embeddings.position_ids', 'text_model.encoder.layer.11.attention.output.dense.weight', 'text_model.encoder.layer.1.attention.self.value.bias', 'text_model.encoder.layer.9.output.dense.weight', 'text_model.encoder.layer.4.output.LayerNorm.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.bias', 'text_model.encoder.layer.7.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.value.weight', 'text_model.encoder.layer.1.output.dense.bias', 'text_model.encoder.layer.0.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.self.key.bias', 'text_model.encoder.layer.4.attention.output.LayerNorm.bias', 'text_model.encoder.layer.6.attention.self.query.bias', 'text_projection.linear2.weight', 'text_model.encoder.layer.9.attention.self.key.bias', 'text_model.encoder.layer.0.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.intermediate.dense.weight', 'text_model.encoder.layer.7.output.LayerNorm.weight', 'text_model.embeddings.LayerNorm.bias', 'text_model.encoder.layer.6.intermediate.dense.weight', 'text_model.encoder.layer.0.output.dense.weight', 'text_model.encoder.layer.10.attention.self.key.bias', 'text_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_model.encoder.layer.0.attention.output.dense.weight', 'logit_scale_t', 'text_model.encoder.layer.10.output.dense.weight', 'text_model.encoder.layer.1.intermediate.dense.weight', 'text_model.encoder.layer.11.attention.self.value.weight', 'text_model.encoder.layer.6.attention.output.dense.bias', 'text_model.encoder.layer.2.attention.self.query.bias', 'text_model.encoder.layer.2.intermediate.dense.bias', 'text_model.encoder.layer.7.output.dense.bias', 'text_model.encoder.layer.0.attention.self.key.weight', 'text_model.encoder.layer.10.intermediate.dense.weight', 'text_model.embeddings.token_type_ids', 'text_model.encoder.layer.3.attention.self.query.weight', 'text_model.encoder.layer.3.attention.self.query.bias', 'text_model.encoder.layer.9.attention.self.value.weight', 'text_model.encoder.layer.4.attention.self.key.bias', 'text_model.encoder.layer.9.intermediate.dense.weight', 'text_model.encoder.layer.0.output.LayerNorm.bias', 'text_model.encoder.layer.5.attention.output.dense.bias', 'text_model.encoder.layer.3.output.LayerNorm.bias', 'text_model.encoder.layer.1.attention.self.query.weight', 'text_model.encoder.layer.11.attention.output.dense.bias', 'text_model.encoder.layer.1.attention.self.key.weight', 'text_model.encoder.layer.5.attention.self.query.bias', 'text_model.encoder.layer.1.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.self.query.weight', 'text_model.encoder.layer.6.output.LayerNorm.weight', 'text_projection.linear1.weight', 'text_model.encoder.layer.10.intermediate.dense.bias', 'text_model.encoder.layer.4.output.dense.weight', 'text_model.encoder.layer.11.output.dense.weight', 'text_model.encoder.layer.8.attention.self.key.bias', 'text_model.encoder.layer.8.intermediate.dense.weight', 'text_model.encoder.layer.1.attention.self.query.bias', 'text_model.encoder.layer.2.output.dense.bias', 'text_model.encoder.layer.8.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.self.key.bias', 'text_model.encoder.layer.5.output.dense.weight', 'text_model.encoder.layer.9.output.LayerNorm.bias', 'text_model.encoder.layer.6.output.dense.weight', 'text_model.encoder.layer.3.attention.self.key.bias', 'text_model.encoder.layer.0.attention.self.key.bias', 'text_model.encoder.layer.3.output.dense.bias', 'text_model.encoder.layer.7.output.LayerNorm.bias', 'text_model.encoder.layer.9.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.output.dense.bias', 'text_model.encoder.layer.2.attention.self.query.weight', 'text_model.encoder.layer.0.attention.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.self.query.weight', 'text_model.encoder.layer.6.intermediate.dense.bias', 'text_model.encoder.layer.8.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.self.query.weight', 'text_model.encoder.layer.7.attention.output.LayerNorm.weight', 'text_model.encoder.layer.9.attention.output.dense.weight', 'text_model.encoder.layer.10.output.LayerNorm.bias', 'text_model.encoder.layer.8.intermediate.dense.bias', 'text_model.encoder.layer.8.attention.self.value.weight', 'text_model.encoder.layer.1.output.dense.weight', 'text_model.encoder.layer.6.attention.self.key.bias', 'text_model.encoder.layer.7.intermediate.dense.bias', 'text_model.encoder.layer.7.attention.self.key.bias', 'text_model.encoder.layer.6.attention.output.LayerNorm.bias', 'text_model.encoder.layer.11.output.LayerNorm.weight', 'text_model.encoder.layer.1.attention.self.value.weight', 'text_model.encoder.layer.5.attention.self.key.weight', 'text_model.encoder.layer.1.attention.output.LayerNorm.bias', 'text_model.encoder.layer.2.attention.self.value.bias', 'text_model.encoder.layer.2.attention.output.LayerNorm.weight', 'text_model.encoder.layer.6.attention.self.query.weight', 'text_model.encoder.layer.2.output.LayerNorm.weight', 'text_model.encoder.layer.5.attention.output.LayerNorm.bias', 'text_model.encoder.layer.10.attention.output.dense.weight', 'text_model.encoder.layer.2.attention.self.key.weight', 'text_model.encoder.layer.3.attention.output.dense.weight', 'text_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_model.encoder.layer.9.intermediate.dense.bias', 'text_model.encoder.layer.6.attention.self.key.weight', 'text_model.encoder.layer.4.attention.output.dense.bias', 'text_model.encoder.layer.0.output.dense.bias', 'text_model.encoder.layer.3.output.dense.weight', 'text_model.encoder.layer.9.attention.self.key.weight', 'text_model.encoder.layer.7.attention.self.query.weight', 'text_model.encoder.layer.2.intermediate.dense.weight', 'text_model.encoder.layer.7.output.dense.weight']
- This IS expected if you are initializing ClapAudioModelWithProjection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing ClapAudioModelWithProjection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Availability of torch:  True
Device for model training & evaluation:  cuda

*** NISQA VAL SIM ***
Dataset({
    features: ['input_values', 'label'],
    num_rows: 2500
})

*** NISQA VAL LIVE ***
Dataset({
    features: ['input_values', 'label'],
    num_rows: 200
})

*** NISQA TEST FOR ***
Dataset({
    features: ['input_values', 'label'],
    num_rows: 240
})

*** NISQA TEST P501 ***
Dataset({
    features: ['input_values', 'label'],
    num_rows: 240
})

*** NISQA TEST LIVETALK ***
Dataset({
    features: ['input_values', 'label'],
    num_rows: 232
})
CLAP model with MOS prediction head:
 CustomModel(
  (base_model): ClapAudioModelWithProjection(
    (audio_model): ClapAudioModel(
      (audio_encoder): ClapAudioEncoder(
        (patch_embed): ClapAudioPatchEmbed(
          (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))
          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
        )
        (layers): ModuleList(
          (0): ClapAudioStage(
            (blocks): ModuleList(
              (0-1): 2 x ClapAudioLayer(
                (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (attention): ClapAudioAttention(
                  (self): ClapAudioSelfAttention(
                    (query): Linear(in_features=96, out_features=96, bias=True)
                    (key): Linear(in_features=96, out_features=96, bias=True)
                    (value): Linear(in_features=96, out_features=96, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): ClapAudioSelfOutput(
                    (dense): Linear(in_features=96, out_features=96, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): Identity()
                (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
                (intermediate): ClapAudioIntermediate(
                  (dense): Linear(in_features=96, out_features=384, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): ClapAudioOutput(
                  (dense): Linear(in_features=384, out_features=96, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (downsample): ClapAudioPatchMerging(
              (reduction): Linear(in_features=384, out_features=192, bias=False)
              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
            )
          )
          (1): ClapAudioStage(
            (blocks): ModuleList(
              (0-1): 2 x ClapAudioLayer(
                (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (attention): ClapAudioAttention(
                  (self): ClapAudioSelfAttention(
                    (query): Linear(in_features=192, out_features=192, bias=True)
                    (key): Linear(in_features=192, out_features=192, bias=True)
                    (value): Linear(in_features=192, out_features=192, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): ClapAudioSelfOutput(
                    (dense): Linear(in_features=192, out_features=192, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): Identity()
                (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
                (intermediate): ClapAudioIntermediate(
                  (dense): Linear(in_features=192, out_features=768, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): ClapAudioOutput(
                  (dense): Linear(in_features=768, out_features=192, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (downsample): ClapAudioPatchMerging(
              (reduction): Linear(in_features=768, out_features=384, bias=False)
              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (2): ClapAudioStage(
            (blocks): ModuleList(
              (0-5): 6 x ClapAudioLayer(
                (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (attention): ClapAudioAttention(
                  (self): ClapAudioSelfAttention(
                    (query): Linear(in_features=384, out_features=384, bias=True)
                    (key): Linear(in_features=384, out_features=384, bias=True)
                    (value): Linear(in_features=384, out_features=384, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): ClapAudioSelfOutput(
                    (dense): Linear(in_features=384, out_features=384, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): Identity()
                (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
                (intermediate): ClapAudioIntermediate(
                  (dense): Linear(in_features=384, out_features=1536, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): ClapAudioOutput(
                  (dense): Linear(in_features=1536, out_features=384, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
            (downsample): ClapAudioPatchMerging(
              (reduction): Linear(in_features=1536, out_features=768, bias=False)
              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
            )
          )
          (3): ClapAudioStage(
            (blocks): ModuleList(
              (0-1): 2 x ClapAudioLayer(
                (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (attention): ClapAudioAttention(
                  (self): ClapAudioSelfAttention(
                    (query): Linear(in_features=768, out_features=768, bias=True)
                    (key): Linear(in_features=768, out_features=768, bias=True)
                    (value): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                  (output): ClapAudioSelfOutput(
                    (dense): Linear(in_features=768, out_features=768, bias=True)
                    (dropout): Dropout(p=0.0, inplace=False)
                  )
                )
                (drop_path): Identity()
                (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
                (intermediate): ClapAudioIntermediate(
                  (dense): Linear(in_features=768, out_features=3072, bias=True)
                  (intermediate_act_fn): GELUActivation()
                )
                (output): ClapAudioOutput(
                  (dense): Linear(in_features=3072, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
            )
          )
        )
        (batch_norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (avgpool): AdaptiveAvgPool1d(output_size=1)
      )
    )
    (audio_projection): ClapProjectionLayer(
      (linear1): Linear(in_features=768, out_features=512, bias=True)
      (activation): ReLU()
      (linear2): Linear(in_features=512, out_features=512, bias=True)
    )
  )
  (mos_head): Linear(in_features=512, out_features=1, bias=True)
)

Testset under consideration:  NISQA_VAL_LIVE
  0%|          | 0/4 [00:00<?, ?it/s] 75%|███████▌  | 3/4 [00:00<00:00, 25.39it/s]100%|██████████| 4/4 [00:00<00:00, 17.74it/s]
RMSE (utterance-level) =  0.42202990831474685
Pearson Correlation Coefficient =  0.8172594899376838
Spearman Rank Correlation Coefficient =  0.8126137198762136

Testset under consideration:  NISQA_TEST_FOR
  0%|          | 0/4 [00:00<?, ?it/s] 75%|███████▌  | 3/4 [00:00<00:00, 25.92it/s]100%|██████████| 4/4 [00:00<00:00, 17.29it/s]
RMSE (utterance-level) =  0.3822550704998981
Pearson Correlation Coefficient =  0.8986086304839963
Spearman Rank Correlation Coefficient =  0.8930051048864539

Testset under consideration:  NISQA_TEST_P501
  0%|          | 0/4 [00:00<?, ?it/s] 75%|███████▌  | 3/4 [00:00<00:00, 25.99it/s]100%|██████████| 4/4 [00:00<00:00, 17.34it/s]
RMSE (utterance-level) =  0.4490534768628211
Pearson Correlation Coefficient =  0.9093595002086994
Spearman Rank Correlation Coefficient =  0.9140295009630438

Testset under consideration:  NISQA_TEST_LIVETALK
  0%|          | 0/4 [00:00<?, ?it/s] 75%|███████▌  | 3/4 [00:00<00:00, 25.86it/s]100%|██████████| 4/4 [00:00<00:00, 17.85it/s]
RMSE (utterance-level) =  0.5954071310128215
Pearson Correlation Coefficient =  0.7736326356244495
Spearman Rank Correlation Coefficient =  0.7477332331043428

Testset under consideration:  NISQA_VAL_SIM
  0%|          | 0/40 [00:00<?, ?it/s]  8%|▊         | 3/40 [00:00<00:01, 27.05it/s] 15%|█▌        | 6/40 [00:01<00:12,  2.76it/s] 20%|██        | 8/40 [00:01<00:08,  3.98it/s] 25%|██▌       | 10/40 [00:02<00:05,  5.44it/s] 30%|███       | 12/40 [00:03<00:10,  2.78it/s] 35%|███▌      | 14/40 [00:03<00:06,  3.79it/s] 40%|████      | 16/40 [00:03<00:04,  5.01it/s] 45%|████▌     | 18/40 [00:03<00:03,  6.46it/s] 50%|█████     | 20/40 [00:05<00:05,  3.38it/s] 55%|█████▌    | 22/40 [00:07<00:09,  1.87it/s] 60%|██████    | 24/40 [00:08<00:09,  1.65it/s] 65%|██████▌   | 26/40 [00:08<00:06,  2.28it/s] 70%|███████   | 28/40 [00:09<00:03,  3.09it/s] 75%|███████▌  | 30/40 [00:09<00:02,  4.11it/s] 80%|████████  | 32/40 [00:09<00:01,  5.35it/s] 85%|████████▌ | 34/40 [00:09<00:00,  6.78it/s] 90%|█████████ | 36/40 [00:09<00:00,  8.32it/s] 95%|█████████▌| 38/40 [00:09<00:00,  9.90it/s]100%|██████████| 40/40 [00:09<00:00,  9.74it/s]100%|██████████| 40/40 [00:09<00:00,  4.04it/s]
RMSE (utterance-level) =  0.4608129882747827
Pearson Correlation Coefficient =  0.908966107247309
Spearman Rank Correlation Coefficient =  0.9050173817242497
